{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "Sub-Task 1:Build churn model(s) to try to predict the churn probability of any customer.\n",
    "\n",
    "Sub-Task 2:Evaluate your model, using a holdout set, and with metrics of your choosing.\n",
    "\n",
    "Sub-Task 3:Interpret the results and use them to formulate answers to the client’s hypotheses and questions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Handle class imbalance\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import (TomekLinks, \n",
    "                                     NeighbourhoodCleaningRule as NCR, \n",
    "                                     RandomUnderSampler)\n",
    "\n",
    "\n",
    "# ML\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Assemble pipeline(s)\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn import set_config\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import make_scorer, classification_report\n",
    "from mlxtend.evaluate import lift_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  #export OMP_NUM_THREADS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plots in jupyter notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set maximum number of columns to be displayed\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JS visualization code to notebook\n",
    "shap.initjs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data (pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/soumyadeepray/My Documents/DS_Projects/BCG_Customer_Churn_Case_Study/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./processed_data.pkl')\n",
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining sampling approaches\n",
    "\n",
    "Churn data sets suffer usually from high-class imbalance. This means that the number of churners are in the minority. To deal with this class imbalance the package imbalanced-learn comes with a battery of different sampling approaches.\n",
    "\n",
    "- `SMOTE - (Synthetic Minority Oversampling Technique)` is an oversampling technique where the synthetic samples are generated for the minority class. \n",
    "\n",
    "- `ADASYN - (Adaptive Synthetic)` is an algorithm that generates synthetic data. Its greatest advantages are not copying the same minority data, and generating more data for “harder to learn\" examples.\n",
    "\n",
    "- `TomekLinks` is used as an undersampling method and removes noisy and borderline majority class examples. The procedure for finding Tomek Links can be used to locate all cross-class nearest neighbors. If the examples in the minority class are held constant, the procedure can be used to find all of those examples in the majority class that are closest to the minority class, then removed.\n",
    "\n",
    "- `NCR - Neighborhood Cleaning rule` is an undersampling technique that combines both the Condensed Nearest Neighbor (CNN) Rule to remove redundant examples and the Edited Nearest Neighbors (ENN) Rule to remove noisy or ambiguous examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store different sampling approaches\n",
    "sampl_app = dict()\n",
    "\n",
    "# No sampling\n",
    "#sampl_app['no_sampling'] = ('no_sampling', None)\n",
    "\n",
    "# SMOTE\n",
    "sampl_app['o_SMOTE'] = ('smote', SMOTE())\n",
    "\n",
    "# ADASYN (Adaptive Synthetic) \n",
    "sampl_app['o_ADASYN'] = ('adasyn', ADASYN(sampling_strategy='not minority'))\n",
    "\n",
    "# TomekLinks\n",
    "sampl_app['u_TomekLinks'] = ('tomeklinks', TomekLinks())\n",
    "\n",
    "# # NCR\n",
    "# sampl_app['u_NCR'] = ('ncr', NCR())\n",
    "\n",
    "# # SMOTE + TomekLinks\n",
    "# sampl_app['h_SMOTE_Tomek'] = imbPipeline([('smote', SMOTE()),\n",
    "#                                           ('tomeklinks', TomekLinks())])\n",
    "\n",
    "# # SMOTE + NCR\n",
    "# sampl_app['h_SMOTE_NCR'] = imbPipeline([('smote', SMOTE()),\n",
    "#                                         ('ncr', \n",
    "#                                          NCR(sampling_strategy='not majority'))]\n",
    "#                                        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use these sampling methods later in the pipeline, they have to be brought in the right format (tuple) first. Approaches that use a combination of multiple sampling methods, have to be wrapped in an `imbPipeline` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampl_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model (logistic regression)\n",
    "lr = LogisticRegression(solver='saga',\n",
    "                            warm_start=True,\n",
    "                            max_iter=1000)\n",
    "\n",
    "# RandomForest\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# XGB\n",
    "xgb = XGBClassifier(tree_method=\"hist\",\n",
    "                        verbosity=0,\n",
    "                        silent=True)\n",
    "\n",
    "# LR, XGB,RF, FFNN\n",
    "lr_xgb_rf = VotingClassifier(estimators=[('lr', lr),\n",
    "                                         ('xgb', xgb),\n",
    "                                         ('rf', rf)\n",
    "                                        ], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store them as tuples in a list\n",
    "classifiers = {'LogisiticRegression': lr,\n",
    "          'RandomForestClassifier': rf,\n",
    "          'XGBClassifier': xgb}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data\n",
    "\n",
    "\n",
    "First of all we will split the data into the variable that we are trying to predict y (churn) and those variables that we will use to predict churn X (the\n",
    "rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"churn\"]\n",
    "X = df.drop(labels = [\"Unnamed: 0\",\"origin_up\", \"id\",\"churn\"],axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will split the data into training and validation data. The percentages of each test can be changed but a 75%-25% is a good ratio.\n",
    "We also use a random state generator in order to split it randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25)\n",
    "print(f\"len of X {len(y)}\\nlen of train {len(y_train)}\\nlen of test {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores to track\n",
    "scorer = {\n",
    "    'lift_score': make_scorer(lift_score),\n",
    "    'roc_auc':'roc_auc', \n",
    "    'f1_macro':'f1_macro', \n",
    "    'recall':'recall'\n",
    "}\n",
    "\n",
    "# To store the performance\n",
    "bnchmrk_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store them as tuples in a list\n",
    "models = [#('lr', lr),\n",
    "          #('rf', rf),\n",
    "          #('xgb', xgb)\n",
    "        #   ('svc',svc),\n",
    "        #   ('gnb', gnb),\n",
    "        #   ('lgb', lgb),\n",
    "        #   ('knn', knn)]\n",
    "        #   ('gev_nn', gev_nn),\n",
    "        #   ('ffnn', ffnn),\n",
    "          ('lr_xgb_rf', lr_xgb_rf)]\n",
    "        #   ('lr_xgb_rf_ffnn', lr_xgb_rf_ffnn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial pipeline\n",
    "ppl = imbPipeline([\n",
    "    ('transformation', ColumnTransformer([\n",
    "        ('num',make_pipeline(\n",
    "            SimpleImputer(strategy='mean'),\n",
    "            MinMaxScaler()),\n",
    "         make_column_selector(dtype_include='number')\n",
    "        ),\n",
    "        ('cat',make_pipeline(\n",
    "            SimpleImputer(strategy='most_frequent'),\n",
    "            OneHotEncoder(sparse=False, handle_unknown='ignore')),\n",
    "         make_column_selector(dtype_include='object')\n",
    "        )])\n",
    "    )\n",
    "])\n",
    "\n",
    "initial_steps = len(ppl.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores to track\n",
    "scorer = {\n",
    "    'lift_score': make_scorer(lift_score),\n",
    "    'roc_auc':'roc_auc', \n",
    "    'f1_macro':'f1_macro', \n",
    "    'recall':'recall'\n",
    "}\n",
    "\n",
    "# To store the performance\n",
    "bnchmrk_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {} #Store model performance\n",
    "\n",
    "for m in models:\n",
    "\n",
    "    sampling_results = {} # Store sampling performance for respective model\n",
    "    for sa in sampl_app.keys():\n",
    "        #logging.info(f\"== Running {m[0]} with {sa} strategy ==\")\n",
    "          \n",
    "        # Extend initial pipeline by sampling approach and model\n",
    "        # Since some sampling approaches have multiple steps \n",
    "        # (e.g., SMOTE + RND) I have to append them via loop\n",
    "        if hasattr(sampl_app[sa], 'steps'):\n",
    "            for s in sampl_app[sa].steps:\n",
    "                ppl.steps.append(s)\n",
    "        else:\n",
    "            ppl.steps.append(sampl_app[sa])\n",
    "            \n",
    "        # Add model to pipeline\n",
    "        ppl.steps.append(m)\n",
    "\n",
    "        # Configure KFold and CV\n",
    "        rsf = RepeatedStratifiedKFold(n_repeats=5, random_state=42)\n",
    "            \n",
    "        scores = cross_validate(ppl, X, y, \n",
    "                                    cv=rsf, \n",
    "                                    scoring=scorer, \n",
    "                                    verbose=0, \n",
    "                                    n_jobs=1,\n",
    "                                    error_score='raise',\n",
    "                                    fit_params=None,\n",
    "                                    return_estimator=False\n",
    "                                   )\n",
    "            \n",
    "        # Write results in dict\n",
    "        sampling_results[sa] = scores\n",
    "            \n",
    "        # After running CV we reset pipeline to initial state\n",
    "        # to be clean for next iteration\n",
    "        ppl = ppl[:initial_steps]\n",
    "        \n",
    "    # Write results in dict\n",
    "    model_results[m[0]] = sampling_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([3.426965  , 3.29950309, 3.36576891, 3.4700439 , 3.36356997,\n",
       "        3.55037904, 3.44505906, 3.60998607, 3.4273479 , 3.45576406,\n",
       "        3.34746909, 3.36455202, 3.64999604, 3.52729607, 3.38929296,\n",
       "        3.32763386, 3.41151786, 3.40283394, 3.42948508, 3.52487588,\n",
       "        3.42055511, 3.57840395, 3.43593502, 3.31255007, 3.63628793]),\n",
       " 'score_time': array([0.10512996, 0.13772273, 0.13218832, 0.12826228, 0.10882497,\n",
       "        0.13108087, 0.12550473, 0.15912294, 0.11616611, 0.13758898,\n",
       "        0.11991286, 0.20744014, 0.17795181, 0.14138913, 0.13046002,\n",
       "        0.14185214, 0.20169711, 0.11568689, 0.12576008, 0.12802505,\n",
       "        0.15417862, 0.15334988, 0.12695098, 0.13796687, 0.15740085]),\n",
       " 'test_lift_score': array([ 8.22816901,  9.55055332,  8.91384977, 10.28521127,  9.17471535,\n",
       "         9.55055332,  8.47017399,  7.34657948, 10.28521127,  9.17471535,\n",
       "         7.91170098,  9.49404117,  9.14241002,  9.14241002,  8.02787593,\n",
       "         7.99960876,  8.81589537,  9.55055332, 10.28521127,  8.14859587,\n",
       "         9.49404117,  8.35673415,  8.47017399,  8.22816901,  6.45097173]),\n",
       " 'test_roc_auc': array([0.74080127, 0.71386873, 0.69023699, 0.69745416, 0.70513319,\n",
       "        0.70725777, 0.70775048, 0.67804724, 0.74530783, 0.70408838,\n",
       "        0.68502139, 0.69569961, 0.71476203, 0.733516  , 0.68823823,\n",
       "        0.69056279, 0.70733254, 0.71816031, 0.70576893, 0.70440182,\n",
       "        0.70673167, 0.71591838, 0.71577016, 0.68870008, 0.68286018]),\n",
       " 'test_f1_macro': array([0.51532772, 0.51909314, 0.51885261, 0.52590239, 0.5025324 ,\n",
       "        0.51909314, 0.52187709, 0.5084849 , 0.51601836, 0.5025324 ,\n",
       "        0.50869258, 0.51578727, 0.5286119 , 0.5286119 , 0.49892756,\n",
       "        0.52162837, 0.51555706, 0.51909314, 0.49565363, 0.52512014,\n",
       "        0.51578727, 0.51861302, 0.52187709, 0.51532772, 0.49187138]),\n",
       " 'test_recall': array([0.04225352, 0.04577465, 0.04577465, 0.0528169 , 0.02826855,\n",
       "        0.04577465, 0.04929577, 0.03521127, 0.04225352, 0.02826855,\n",
       "        0.03521127, 0.04225352, 0.05633803, 0.05633803, 0.02473498,\n",
       "        0.04929577, 0.04225352, 0.04577465, 0.02112676, 0.05300353,\n",
       "        0.04225352, 0.04577465, 0.04929577, 0.04225352, 0.01766784])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results['lr_xgb_rf']['o_ADASYN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results['xgb']['o_SMOTE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_df = pd.DataFrame.from_dict(model_results['xgb'])\n",
    "model_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
